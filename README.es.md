![License: MIT](https://img.shields.io/badge/Licence-MIT-green)
![University: Paris 8](https://img.shields.io/badge/University-Paris%208-red)
![deep: learning](https://img.shields.io/badge/deep-learning-blue)
![python: 3.12](https://img.shields.io/badge/python-3.12-brightgreen)
![Contributors](https://img.shields.io/badge/contributor-2-orange)
![Stars](https://img.shields.io/github/stars/Fab16BSB/image_classification?color=orange)
![Fork](https://img.shields.io/github/forks/Fab16BSB/image_classification?color=orange)
![Watchers](https://img.shields.io/github/watchers/Fab16BSB/image_classification?color=orange)


# Proyecto de ClasificaciÃ³n de ImÃ¡genes con CNN

## ğŸŒ Versiones MultilingÃ¼es del README

- ğŸ‡«ğŸ‡· [FrancÃ©s](./README.fr.md)
- ğŸ‡¬ğŸ‡§ [InglÃ©s](./README.md)
- ğŸ‡ªğŸ‡¸ [EspaÃ±ol (estÃ¡s aquÃ­)](#)

---

## ğŸ“˜ Resumen del Proyecto

Este proyecto, realizado como parte de mi MÃ¡ster, tiene como objetivo crear un **sistema de clasificaciÃ³n de imÃ¡genes** utilizando **Redes Neuronales Convolucionales (CNNs)** con **Keras**. El proyecto implica el raspado de imÃ¡genes para construir conjuntos de datos, seguido del entrenamiento de modelos CNN para la clasificaciÃ³n.

---

## ğŸ“ Estructura del Proyecto

Este proyecto estÃ¡ organizado en varias carpetas principales, cada una conteniendo elementos especÃ­ficos:

* `code/`: Esta carpeta reÃºne todos los scripts de Python y notebooks de Jupyter dedicados a la **creaciÃ³n**, **entrenamiento** y **carga** de modelos de clasificaciÃ³n.
* `data/`: Esta carpeta contiene los diversos **conjuntos de datos** del proyecto, incluyendo **Dragon Ball**, **Fairy Tail** y **PokÃ©mon**, organizados para entrenamiento, validaciÃ³n y predicciÃ³n.
* `modele/`: Esta carpeta almacena los **modelos CNN entrenados y guardados** para cada conjunto de datos, incluyendo su arquitectura, pesos y etiquetas de clase asociadas.

```
code/
â”œâ”€â”€ Creer_model.ipynb   # Notebook de Jupyter para la creaciÃ³n del modelo
â”œâ”€â”€ creer_model.py      # Script de Python para la creaciÃ³n del modelo
â”œâ”€â”€ Load_model.ipynb    # Notebook de Jupyter para la carga del modelo
â””â”€â”€ load_model.py       # Script de Python para la carga del modelo
data/
â”œâ”€â”€ dragonball/
â”‚   â”œâ”€â”€ predict/    # ImÃ¡genes para predicciÃ³n
â”‚   â”œâ”€â”€ train/      # ImÃ¡genes para entrenamiento
â”‚   â”‚   â”œâ”€â”€ gohan/
â”‚   â”‚   â”œâ”€â”€ goku/
â”‚   â”‚   â”œâ”€â”€ piccolo/
â”‚   â”‚   â””â”€â”€ vegeta/
â”‚   â””â”€â”€ validation/ # ImÃ¡genes para validaciÃ³n
â”‚       â”œâ”€â”€ gohan/
â”‚       â”œâ”€â”€ goku/
â”‚       â”œâ”€â”€ piccolo/
â”‚       â””â”€â”€ vegeta/
â”œâ”€â”€ fairytail/
â”‚   â”œâ”€â”€ predict/    # ImÃ¡genes para predicciÃ³n
â”‚   â”œâ”€â”€ train/      # ImÃ¡genes para entrenamiento
â”‚   â”‚   â”œâ”€â”€ erza_scarlet/
â”‚   â”‚   â”œâ”€â”€ grey_fullbuster/
â”‚   â”‚   â”œâ”€â”€ lucy_heartfilia/
â”‚   â”‚   â””â”€â”€ natsu_dragneel/
â”‚   â””â”€â”€ validation/ # ImÃ¡genes para validaciÃ³n
â”‚       â”œâ”€â”€ erza_scarlet/
â”‚       â”œâ”€â”€ grey_fullbuster/
â”‚       â”œâ”€â”€ lucy_heartfilia/
â”‚       â””â”€â”€ natsu_dragneel/
â””â”€â”€ pokemon/
â”œâ”€â”€ predict/    # ImÃ¡genes para predicciÃ³n
â”œâ”€â”€ train/      # ImÃ¡genes para entrenamiento
â”‚   â”œâ”€â”€ bulbasaur/
â”‚   â”œâ”€â”€ charmander/
â”‚   â”œâ”€â”€ pikachu/
â”‚   â””â”€â”€ squirtle/
â””â”€â”€ validation/ # ImÃ¡genes para validaciÃ³n
â”œâ”€â”€ bulbasaur/
â”œâ”€â”€ charmander/
â”œâ”€â”€ pikachu/
â””â”€â”€ squirtle/
modele/
â”œâ”€â”€ dragonball_model/
â”‚   â”œâ”€â”€ architecture.json  # Arquitectura del modelo
â”‚   â”œâ”€â”€ poids.h5           # Pesos del modelo
â”‚   â””â”€â”€ labels.txt         # Etiquetas de clase
â”œâ”€â”€ fairytail_model/
â”‚   â”œâ”€â”€ architecture.json  # Arquitectura del modelo
â”‚   â”œâ”€â”€ poids.h5           # Pesos del modelo
â”‚   â””â”€â”€ labels.txt         # Etiquetas de clase
â””â”€â”€ pokemon_model/
â”œâ”€â”€ architecture.json  # Arquitectura del modelo
â”œâ”€â”€ poids.h5           # Pesos del modelo
â””â”€â”€ labels.txt         # Etiquetas de clase

```

---

## ğŸ“Š Conjuntos de Datos

Para este proyecto, creamos **tres conjuntos de datos distintos**, cada uno compuesto por **cuatro clases**. Las imÃ¡genes se obtuvieron principalmente de **Google Images** y **Anime Characters Database**.

* **Dragon Ball**
    * Entrenamiento: Gohan (34), Goku (42), Piccolo (20), Vegeta (44)
    * ValidaciÃ³n: Gohan (5), Goku (21), Piccolo (7), Vegeta (15)


* **Fairy Tail**
    * Entrenamiento: Erza Scarlet (22), Grey Fullbuster (15), Lucy Heartfilia (25), Natsu Dragneel (13)
    * ValidaciÃ³n: Erza Scarlet (5), Grey Fullbuster (5), Lucy Heartfilia (6), Natsu Dragneel (6)


* **PokÃ©mon**
    * Entrenamiento: Bulbasaur (100), Charmander (96), Pikachu (156), Squirtle (102)
    * ValidaciÃ³n: Bulbasaur (30), Charmander (23), Pikachu (38), Squirtle (28)

---

## ğŸ–¼ï¸ Aumento de Datos (Data Augmentation)

El aumento de datos es una tÃ©cnica esencial para **mejorar la robustez del modelo** y **prevenir el sobreajuste**, creando nuevas imÃ¡genes de entrenamiento a partir de las existentes. A continuaciÃ³n, se presentan las transformaciones aplicadas en este proyecto:

* ğŸ“ **Reescalado**: Los valores de los pÃ­xeles de las imÃ¡genes se escalan a un rango entre 0 y 1. Este es un paso crucial de **normalizaciÃ³n** para el procesamiento de la red neuronal.


* âœ‚ï¸ **Corte (Shear Range)**: Las imÃ¡genes se "sesgan" o distorsionan a lo largo de un eje. Esto ayuda al modelo a reconocer objetos desde **varios Ã¡ngulos o perspectivas**.


* ğŸ” **Zoom (Zoom Range)**: Se aplican zooms aleatorios a las imÃ¡genes. Esta transformaciÃ³n permite al modelo identificar mejor las caracterÃ­sticas, independientemente de su **tamaÃ±o o distancia relativa** dentro de la imagen.


* â†”ï¸ **Volteo Horizontal (Horizontal Flip)**: Las imÃ¡genes se voltean horizontalmente de forma aleatoria. Esto es particularmente Ãºtil para objetos que pueden aparecer en cualquier **orientaciÃ³n lateral**, como la mayorÃ­a de los personajes u objetos.

---

## âš™ï¸ CÃ³mo Funciona una CNN

Una **Red Neuronal Convolucional (CNN)** es un tipo de red neuronal diseÃ±ado especÃ­ficamente para procesar datos con una estructura de cuadrÃ­cula, como las imÃ¡genes. Su eficacia radica en su capacidad para aprender automÃ¡ticamente caracterÃ­sticas (o "features") relevantes directamente de las imÃ¡genes, sin intervenciÃ³n humana.

AsÃ­ es como opera, paso a paso:

* ğŸ” **ExtracciÃ³n de CaracterÃ­sticas (Capas Convolucionales)**: En el corazÃ³n de una CNN se encuentran las capas convolucionales. Estas aplican "filtros" (pequeÃ±as matrices de nÃºmeros) a travÃ©s de la imagen. Cada filtro estÃ¡ diseÃ±ado para detectar un tipo especÃ­fico de patrÃ³n, como **bordes**, **texturas** o **formas simples**. Al deslizar este filtro por toda la imagen, la capa convolucional crea un "mapa" que indica dÃ³nde estÃ¡n presentes estos patrones y con quÃ© intensidad. La red aprende progresivamente quÃ© filtros son mÃ¡s Ãºtiles para la tarea dada.


* ğŸ”½ **ReducciÃ³n de Complejidad (Capas de Pooling)**: DespuÃ©s de la convoluciÃ³n, entran en juego las capas de *pooling*. Su funciÃ³n es **reducir la dimensionalidad** de los mapas de caracterÃ­sticas. Agregan informaciÃ³n reteniendo solo los valores mÃ¡s significativos (por ejemplo, el valor mÃ¡ximo en una pequeÃ±a regiÃ³n). Esto ayuda a que el modelo sea mÃ¡s robusto a pequeÃ±as variaciones en la posiciÃ³n de los patrones y a reducir el costo computacional.


* ğŸ§  **Aprendizaje de Patrones Complejos y ClasificaciÃ³n (Capas Densas)**: La informaciÃ³n procesada y simplificada por las capas convolucionales y de pooling es luego "aplanada" y alimentada a capas de neuronas llamadas "densas" o "totalmente conectadas". AquÃ­ es donde la red aprende a **combinar caracterÃ­sticas de alto nivel** detectadas anteriormente para identificar patrones mÃ¡s complejos y, finalmente, tomar una **decisiÃ³n de clasificaciÃ³n**. Por ejemplo, despuÃ©s de detectar ojos y una nariz, las capas densas aprenderÃ¡n a ensamblarlos para reconocer una cara especÃ­fica.

Al repetir este proceso a travÃ©s de varias capas convolucionales y de pooling, seguidas de capas densas, la CNN construye una **representaciÃ³n jerÃ¡rquica** de la imagen, pasando de las caracterÃ­sticas mÃ¡s simples (bordes) a las mÃ¡s complejas (objetos enteros), lo que le permite clasificar imÃ¡genes con alta precisiÃ³n.

---

## ğŸ§± Arquitectura de la CNN

AquÃ­ se muestra un diagrama de la arquitectura de nuestro modelo CNN, que ilustra el flujo de datos a travÃ©s de sus diferentes capas:

![Diagrama de Arquitectura de la CNN](result/cnn.png)

---

## ğŸ§ª Resultados y Enfoque Experimental

Este proyecto siguiÃ³ un enfoque experimental progresivo, con el objetivo de explorar las capacidades y limitaciones de las CNN en la clasificaciÃ³n de imÃ¡genes de personajes de manga y anime.

### 1. ClasificaciÃ³n de PokÃ©mon

Comenzamos entrenando nuestros modelos con el conjunto de datos de PokÃ©mon, compuesto por cuatro clases (Bulbasaur, Charmander, Pikachu, Squirtle). El objetivo inicial era validar la arquitectura bÃ¡sica de la CNN y obtener un rendimiento satisfactorio en una tarea relativamente simple.

![Imagen de PredicciÃ³n de PokÃ©mon](result/pokemon.png)

*Ejemplo de predicciones exitosas en imÃ¡genes de PokÃ©mon.*

### 2. DetecciÃ³n de Evoluciones

Luego, probamos la capacidad del modelo para generalizar a imÃ¡genes de evoluciones de PokÃ©mon (por ejemplo, Bulbasaur, Ivysaur, Venusaur). La idea era ver si el modelo podÃ­a atribuir correctamente una imagen a la clase de su pre-evoluciÃ³n, incluso si la imagen mostraba diferencias visuales significativas.

![Imagen de PredicciÃ³n de Evoluciones de PokÃ©mon](result/florizarre.png)

*Resultados de la clasificaciÃ³n de evoluciones de PokÃ©mon.*

### 3. DetecciÃ³n MÃºltiple y Fusiones en Dragon Ball

Pasando a un conjunto de datos mÃ¡s complejo (Dragon Ball), exploramos la detecciÃ³n de mÃºltiples personajes dentro de la misma imagen.

![Imagen de DetecciÃ³n MÃºltiple en Dragon Ball](result/goku_et_vegeta.png)

*Ejemplos de detecciÃ³n de mÃºltiples personajes en la misma imagen.*

TambiÃ©n probamos el modelo con imÃ¡genes de fusiones (como Gogeta), donde un personaje estÃ¡ visualmente compuesto por partes de otros dos (Goku y Vegeta). El objetivo era ver si el modelo podÃ­a identificar similitudes con los personajes "padres".

![FusiÃ³n de Gogeta de Goku + Vegeta](result/gogeta.png)

![Imagen de PredicciÃ³n de FusiÃ³n (Gogeta)](result/prediction_gogeta.png)
![Imagen de PredicciÃ³n de FusiÃ³n (Gogeta) 2](result/prediction_gogeta_2.png)

*PredicciÃ³n en imÃ¡genes de Gogeta.*

Finalmente, probamos el modelo con imÃ¡genes de Trunks (el hijo de Vegeta) para ver si podÃ­a detectar rasgos comunes con su padre, a pesar de las notables diferencias visuales.

![Imagen de PredicciÃ³n de Trunks](result/prediction_trunk.png)
![Imagen de PredicciÃ³n de Trunks 2](result/prediction_trunk_2.png)

*PredicciÃ³n en imÃ¡genes de Trunks.*

### 4. Explicabilidad de los Resultados

Para comprender mejor las decisiones del modelo, exploramos tÃ©cnicas de explicabilidad.

* **VisualizaciÃ³n de Mapas de CaracterÃ­sticas**: ExtraÃ­mos los *mapas de caracterÃ­sticas* de las capas convolucionales para visualizar los patrones que el modelo aprendiÃ³ a detectar. Por ejemplo, pudimos observar mapas que resaltaban la silueta general de un personaje, o partes especÃ­ficas como brazos o piernas.

    ![Imagen de Mapas de CaracterÃ­sticas](result/feature_map.png)

    *Ejemplo de mapas de caracterÃ­sticas que muestran la extracciÃ³n de patrones en una imagen de manga.*


* **Mapas de Salience**: Generamos mapas de salience, que indican las regiones mÃ¡s importantes de la imagen para la predicciÃ³n del modelo. Aplicamos esta tÃ©cnica a imÃ¡genes de Dragon Ball.

    ![Imagen de Mapa de Salience de Dragon Ball](result/filtre_dbz.png)
    ![Imagen de Mapa de Salience de Dragon Ball 2](result/filtre_dbz_2.png)

    *Mapa de salience para imÃ¡genes de Dragon Ball.*

    
AdemÃ¡s, visualizamos los mapas de salience para cada bloque convolucional durante la predicciÃ³n de un personaje de Fairy Tail, para observar cÃ³mo evolucionan las regiones importantes a medida que la CNN procesa la imagen.

![Imagen de Mapas de Salience por Bloque Convolucional](result/prediction_erza.png)

*Mapas de salience para cada bloque convolucional durante la predicciÃ³n de un personaje de Fairy Tail.*

---

## ğŸ’» TecnologÃ­as Utilizadas

* **Lenguaje:** Python
* **LibrerÃ­as:** Keras

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

Para iniciar este proyecto y utilizar los modelos de clasificaciÃ³n de imÃ¡genes, sigue estos pasos:

1.  **Clonar el repositorio**:
    ```
    git clone https://github.com/Fab16BSB/image_classification.git
    ```

2.  **Instalar las dependencias**:
    ```
    cd image_classification
    pip install -r requirements.txt
    ```

3.  **Ejecutar el cÃ³digo**:

* Para **crear y entrenar un nuevo modelo**, utiliza los archivos de la carpeta `code/`:
    * Desde Jupyter Notebook: Abre `Creer_model.ipynb` y ejecuta las celdas.
    * Desde un script de Python: Ejecuta `python code/creer_model.py` desde la raÃ­z del proyecto.


* Para **cargar un modelo existente y hacer predicciones**, utiliza los archivos de la carpeta `code/`:
    * Desde Jupyter Notebook: Abre `Load_model.ipynb` y ejecuta las celdas.
    * Desde un script de Python: Ejecuta `python code/load_model.py` desde la raÃ­z del proyecto.

---

## ğŸ§‘â€ğŸ’» Autores

* **Zeineb Ghrib**: ContribuyÃ³ a la construcciÃ³n del conjunto de datos de PokÃ©mon y participÃ³ en la creaciÃ³n del modelo CNN.

---

## ğŸ“š Fuentes

Los datos utilizados para este proyecto fueron recolectados de las siguientes plataformas, con fuentes especÃ­ficas para cada conjunto de datos:

* **Google Images**: Utilizado para el raspado del conjunto de datos de PokÃ©mon.
* **Anime Characters Database**: La fuente utilizada para los conjuntos de datos de Dragon Ball y Fairy Tail.