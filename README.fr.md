![License: MIT](https://img.shields.io/badge/Licence-MIT-green)
![University: Paris 8](https://img.shields.io/badge/University-Paris%208-red)
![deep: learning](https://img.shields.io/badge/deep-learning-blue)
![python: 3.12](https://img.shields.io/badge/python-3.12-brightgreen)
![Contributors](https://img.shields.io/badge/contributor-2-orange)
![Stars](https://img.shields.io/github/stars/Fab16BSB/image_classification?color=orange)
![Fork](https://img.shields.io/github/forks/Fab16BSB/image_classification?color=orange)
![Watchers](https://img.shields.io/github/watchers/Fab16BSB/image_classification?color=orange)

# Projet de Classification d'Images avec CNN

## ğŸŒ Versions multilingues du README

- ğŸ‡«ğŸ‡· [FranÃ§ais (vous Ãªtes ici)](#)
- ğŸ‡¬ğŸ‡§ [English](./README.md)
- ğŸ‡ªğŸ‡¸ [EspaÃ±ol](./README.es.md)

---

 ## ğŸ“˜ AperÃ§u du Projet

 Ce projet, rÃ©alisÃ© dans le cadre de mon Master, a pour objectif de crÃ©er un systÃ¨me de classification d'images en utilisant des rÃ©seaux de neurones convolutifs (CNN) avec Keras. Le projet implique le scraping d'images pour constituer des datasets, puis l'entraÃ®nement de modÃ¨les CNN pour la classification.

 ---

 ## ğŸ“ Structure du Projet
Ce projet est organisÃ© en plusieurs dossiers, chacun contenant des Ã©lÃ©ments spÃ©cifiques : 

- code: Ce dossier rassemble tous les scripts Python et notebooks Jupyter dÃ©diÃ©s Ã  la crÃ©ation, l'entraÃ®nement, et le chargement des modÃ¨les de classification.
- data: Ce dossier contient les diffÃ©rents jeux de donnÃ©es du projet, incluant Dragon Ball, Fairy Tail, et PokÃ©mon, organisÃ©s pour l'entraÃ®nement, la validation et la prÃ©diction.
- modele: Ce dossier stocke les modÃ¨les de CNN entraÃ®nÃ©s et sauvegardÃ©s pour chaque jeu de donnÃ©es, comprenant leur architecture, leurs poids, et les Ã©tiquettes de classes associÃ©es.

```
code/
â”œâ”€â”€ Creer_model.ipynb   # Notebook Jupyter pour la crÃ©ation du modÃ¨le
â”œâ”€â”€ creer_model.py      # Script Python pour la crÃ©ation du modÃ¨le
â”œâ”€â”€ Load_model.ipynb    # Notebook Jupyter pour le chargement du modÃ¨le
â””â”€â”€ load_model.py       # Script Python pour le chargement du modÃ¨le
data/
â”œâ”€â”€ dragonball/
â”‚   â”œâ”€â”€ predict/    # Images Ã  prÃ©dire
â”‚   â”œâ”€â”€ train/      # Images d'entraÃ®nement
â”‚   â”‚   â”œâ”€â”€ gohan/
â”‚   â”‚   â”œâ”€â”€ goku/
â”‚   â”‚   â”œâ”€â”€ piccolo/
â”‚   â”‚   â””â”€â”€ vegeta/
â”‚   â””â”€â”€ validation/ # Images de validation
â”‚       â”œâ”€â”€ gohan/
â”‚       â”œâ”€â”€ goku/
â”‚       â”œâ”€â”€ piccolo/
â”‚       â””â”€â”€ vegeta/
â”œâ”€â”€ fairytail/
â”‚   â”œâ”€â”€ predict/    # Images Ã  prÃ©dire
â”‚   â”œâ”€â”€ train/      # Images d'entraÃ®nement
â”‚   â”‚   â”œâ”€â”€ erza_scarlet/
â”‚   â”‚   â”œâ”€â”€ grey_fullbuster/
â”‚   â”‚   â”œâ”€â”€ lucy_heartfilia/
â”‚   â”‚   â””â”€â”€ natsu_dragneel/
â”‚   â””â”€â”€ validation/ # Images de validation
â”‚       â”œâ”€â”€ erza_scarlet/
â”‚       â”œâ”€â”€ grey_fullbuster/
â”‚       â”œâ”€â”€ lucy_heartheartfilia/
â”‚       â””â”€â”€ natsu_dragneel/
â””â”€â”€ pokemon/
â”œâ”€â”€ predict/    # Images Ã  prÃ©dire
â”œâ”€â”€ train/      # Images d'entraÃ®nement
â”‚   â”œâ”€â”€ bulbasaur/
â”‚   â”œâ”€â”€ charmander/
â”‚   â”œâ”€â”€ pikachu/
â”‚   â””â”€â”€ squirtle/
â””â”€â”€ validation/ # Images de validation
â”œâ”€â”€ bulbasaur/
â”œâ”€â”€ charmander/
â”œâ”€â”€ pikachu/
â””â”€â”€ squirtle/
modele/
â”œâ”€â”€ dragonball_model/
â”‚   â”œâ”€â”€ architecture.json  # Architecture du modÃ¨le
â”‚   â”œâ”€â”€ poids.h5           # Poids du modÃ¨le
â”‚   â””â”€â”€ labels.txt         # Labels des classes
â”œâ”€â”€ fairytail_model/
â”‚   â”œâ”€â”€ architecture.json  # Architecture du modÃ¨le
â”‚   â”œâ”€â”€ poids.h5           # Poids du modÃ¨le
â”‚   â””â”€â”€ labels.txt         # Labels des classes
â””â”€â”€ pokemon_model/
â”œâ”€â”€ architecture.json  # Architecture du modÃ¨le
â”œâ”€â”€ poids.h5           # Poids du modÃ¨le
â””â”€â”€ labels.txt         # Labels des classes
```


---

## ğŸ“Š Datasets
Pour ce projet, nous avons constituÃ© **trois datasets distincts**, chacun composÃ© de **quatre classes**. Les images ont Ã©tÃ© rÃ©cupÃ©rÃ©es (scrappÃ©es) principalement depuis **Google Images** et **Anime Characters Database**.

* **Dragon Ball**
    * Train: Gohan (34), Goku (42), Piccolo (20), Vegeta (44)
    * Validation: Gohan (5), Goku (21), Piccolo (7), Vegeta (15)


* **Fairy Tail**
    * Train: Erza Scarlet (22), Grey Fullbuster (15), Lucy Heartfilia (25), Natsu Dragneel (13)
    * Validation: Erza Scarlet (5), Grey Fullbuster (5), Lucy Heartfilia (6), Natsu Dragneel (6)


* **PokÃ©mon**
    * Train: Bulbasaur (100), Charmander (96), Pikachu (156), Squirtle (102)
    * Validation: Bulbasaur (30), Charmander (23), Pikachu (38), Squirtle (28)

---

## ğŸ–¼ï¸ Data Augmentation
La data augmentation est une technique essentielle pour amÃ©liorer la robustesse des modÃ¨les et prÃ©venir le surapprentissage, en crÃ©ant de nouvelles images d'entraÃ®nement Ã  partir de celles existantes. Voici les transformations appliquÃ©es dans ce projet :

ğŸ“ Redimensionnement (Rescale): Les valeurs des pixels des images sont ramenÃ©es entre 0 et 1. C'est une Ã©tape de normalisation cruciale pour le traitement par le rÃ©seau de neurones.

âœ‚ï¸ Cisaillement (Shear Range): Les images sont "inclinÃ©es" ou dÃ©formÃ©es le long d'un axe. Cela aide le modÃ¨le Ã  reconnaÃ®tre les objets sous des angles ou perspectives variÃ©s.

ğŸ” Zoom (Zoom Range): Des zooms alÃ©atoires sont appliquÃ©s sur les images. Cette transformation permet au modÃ¨le de mieux identifier les caractÃ©ristiques, quelle que soit leur taille ou leur distance relative dans l'image.

â†”ï¸ Retournement Horizontal (Horizontal Flip): Les images sont retournÃ©es horizontalement de maniÃ¨re alÃ©atoire. Cela est particuliÃ¨rement utile pour des objets qui peuvent apparaÃ®tre dans n'importe quelle orientation latÃ©rale, comme la plupart des personnages ou objets.

---

## âš™ï¸ Fonctionnement d'un rÃ©seau de neuronne de type CNN

 Un **rÃ©seau de neurones convolutif (CNN)** est un type de rÃ©seau de neurones spÃ©cialement conÃ§u pour traiter des donnÃ©es ayant une structure de grille, comme les images. Son efficacitÃ© rÃ©side dans sa capacitÃ© Ã  apprendre automatiquement les caractÃ©ristiques (ou "features") pertinentes directement Ã  partir des images, sans intervention humaine.

 Voici comment il opÃ¨re, Ã©tape par Ã©tape :

 * ğŸ” **Extraction de caractÃ©ristiques (Couches de Convolution)** : Au cÅ“ur du CNN se trouvent les couches de convolution. Elles appliquent des "filtres" (de petites matrices de nombres) sur l'image. Chaque filtre est conÃ§u pour dÃ©tecter un type spÃ©cifique de motif, comme des **bords**, des **textures** ou des **formes simples**. En glissant ce filtre sur toute l'image, la couche de convolution crÃ©e une "carte" qui indique oÃ¹ ces motifs sont prÃ©sents et avec quelle intensitÃ©. Le rÃ©seau apprend progressivement quels filtres sont les plus utiles pour la tÃ¢che donnÃ©e.


 * ğŸ”½ **RÃ©duction de la complexitÃ© (Couches de Pooling)** : AprÃ¨s la convolution, les couches de *pooling* interviennent. Leur rÃ´le est de **rÃ©duire la dimensionnalitÃ©** des cartes de caractÃ©ristiques. Elles agrÃ¨gent l'information en ne conservant que les valeurs les plus significatives (par exemple, la valeur maximale dans une petite rÃ©gion). Cela aide Ã  rendre le modÃ¨le plus robuste aux petites variations de position des motifs et Ã  diminuer le coÃ»t computationnel.


 * ğŸ§  **Apprentissage de motifs complexes et classification (Couches Denses)** : Les informations traitÃ©es et simplifiÃ©es par les couches de convolution et de pooling sont ensuite "aplaties" et alimentent des couches de neurones dites "denses" ou "entiÃ¨rement connectÃ©es". C'est ici que le rÃ©seau apprend Ã  **combiner les caractÃ©ristiques de haut niveau** dÃ©tectÃ©es prÃ©cÃ©demment pour identifier des motifs plus complexes et, finalement, prendre une **dÃ©cision de classification**. Par exemple, aprÃ¨s avoir dÃ©tectÃ© des yeux et un nez, les couches denses apprendront Ã  les assembler pour reconnaÃ®tre un visage spÃ©cifique.

 En rÃ©pÃ©tant ce processus Ã  travers plusieurs couches de convolution et de pooling, suivies de couches denses, le CNN construit une **reprÃ©sentation hiÃ©rarchique** de l'image, allant des caractÃ©ristiques les plus simples (bords) aux plus complexes (objets entiers), ce qui lui permet de classer les images avec une grande prÃ©cision.

 ---

## ğŸ§± Architecture du CNN 

Voici le schÃ©ma de l'architecture de notre modÃ¨le CNN, illustrant le flux des donnÃ©es Ã  travers les diffÃ©rentes couches :

![SchÃ©ma de l'Architecture du CNN](result/cnn.png)

---

## ğŸ§ª RÃ©sultats et DÃ©marche ExpÃ©rimentale

Ce projet a suivi une dÃ©marche expÃ©rimentale progressive, visant Ã  explorer les capacitÃ©s et les limites des CNN dans la classification d'images de personnages de manga et d'anime.

### 1. Classification de PokÃ©mon

Nous avons commencÃ© par entraÃ®ner nos modÃ¨les sur le dataset PokÃ©mon, composÃ© de quatre classes (Bulbasaur, Charmander, Pikachu, Squirtle). L'objectif initial Ã©tait de valider l'architecture de base du CNN et d'obtenir des performances satisfaisantes sur une tÃ¢che relativement simple.

![Image de prÃ©dictions PokÃ©mon](result/pokemon.png)

*Exemple de prÃ©dictions rÃ©ussies sur des images de PokÃ©mon.*

### 2. DÃ©tection d'Ã‰volutions

Nous avons ensuite testÃ© la capacitÃ© du modÃ¨le Ã  gÃ©nÃ©raliser sur des images d'Ã©volutions de PokÃ©mon (par exemple, Bulbizarre, Herbizarre, Florizarre). L'idÃ©e Ã©tait de voir si le modÃ¨le pouvait attribuer correctement une image Ã  la classe de sa prÃ©-Ã©volution, mÃªme si l'image prÃ©sentait des diffÃ©rences visuelles significatives.

![Image de prÃ©dictions d'Ã©volutions de PokÃ©mon](result/florizarre.png)

*RÃ©sultats de la classification d'Ã©volutions de PokÃ©mon.*

### 3. DÃ©tection Multiple et Fusions dans Dragon Ball

Passant Ã  un dataset plus complexe (Dragon Ball), nous avons explorÃ© la dÃ©tection de plusieurs personnages dans une mÃªme image.

![Image de dÃ©tection multiple dans Dragon Ball](result/goku_et_vegeta.png)

*Exemples de dÃ©tection de plusieurs personnages dans une mÃªme image.*

Nous avons Ã©galement testÃ© le modÃ¨le sur des images de fusions (comme Gogeta), oÃ¹ un personnage est visuellement composÃ© de parties de deux autres (Goku et Vegeta). L'objectif Ã©tait de voir si le modÃ¨le pouvait identifier les similaritÃ©s avec les personnages "parents".

![Gogeta fusion de goku + vegeta](result/gogeta.png)

![Image de prÃ©diction de fusion (Gogeta)](result/prediction_gogeta.png)
![Image de prÃ©diction de fusion (Gogeta)](result/prediction_gogeta_2.png)

*PrÃ©diction sur des images de Gogeta.*

Enfin, nous avons testÃ© le modÃ¨le sur des images de Trunks (le fils de Vegeta), pour voir s'il pouvait dÃ©tecter des traits communs avec son pÃ¨re, malgrÃ© des diffÃ©rences visuelles notables.

![Image de prÃ©diction de Trunks](result/prediction_trunk.png)
![Image de prÃ©diction de Trunks](result/prediction_trunk_2.png)

*PrÃ©diction sur des images de Trunks.*

### 4. ExplicabilitÃ© des RÃ©sultats

Pour mieux comprendre les dÃ©cisions du modÃ¨le, nous avons explorÃ© des techniques d'explicabilitÃ©.

* **Visualisation de Feature Maps**: Nous avons extrait les *feature maps* (cartes de caractÃ©ristiques) des couches convolutives pour visualiser les motifs que le modÃ¨le a appris Ã  dÃ©tecter. Par exemple, nous avons pu observer des cartes mettant en Ã©vidence la silhouette gÃ©nÃ©rale d'un personnage, ou des parties spÃ©cifiques comme les bras ou les jambes.

    ![Image de feature maps](result/feature_map.png)

    *Exemple de feature maps montrant l'extraction de motifs dans une image de manga.*


* **Cartes de Saillance**: Nous avons gÃ©nÃ©rÃ© des cartes de saillance, qui indiquent les rÃ©gions de l'image les plus importantes pour la prÃ©diction du modÃ¨le. Nous avons appliquÃ© cette technique sur des images de Dragon Ball.

    ![Image de carte de saillance sur Dragon Ball](result/filtre_dbz.png)
    ![Image de carte de saillance sur Dragon Ball](result/filtre_dbz_2.png)

    *Carte de saillance pour des images de Dragon Ball.*


  De plus, nous avons visualisÃ© les cartes de saillance pour chaque bloc de convolution lors de la prÃ©diction d'un personnage de Fairy Tail, afin d'observer comment les rÃ©gions importantes Ã©voluent au fur et Ã  mesure du traitement par le CNN.

  ![Image de cartes de saillance par bloc de convolution](result/prediction_erza.png)

  *Cartes de saillance pour chaque bloc de convolution lors de la prÃ©diction d'un personnage de Fairy Tail.*

---

## ğŸ’» Technologies UtilisÃ©es

* **Langage:** Python
* **Librairies:** Keras

---

## ğŸš€ Comment ExÃ©cuter le Projet
Pour lancer ce projet et utiliser les modÃ¨les de classification d'images, suivez ces Ã©tapes :

1. Cloner le dÃ©pÃ´t :
```
git clone https://github.com/Fab16BSB/image_classification.git
```


2. Installer les dÃ©pendances :
```
cd image_classification
pip install -r requirements.txt
```

3. **ExÃ©cuter le code** :

* Pour **crÃ©er et entraÃ®ner un nouveau modÃ¨le**, utilisez les fichiers du dossier `code/` :
    * Via le notebook Jupyter : Ouvrez `Creer_model.ipynb` et exÃ©cutez les cellules.
    * Via le script Python : Lancez `python code/creer_model.py` depuis la racine du projet.


* Pour **charger un modÃ¨le existant et faire des prÃ©dictions**, utilisez les fichiers du dossier `code/` :
    * Via le notebook Jupyter : Ouvrez `Load_model.ipynb` et exÃ©cutez les cellules.
    * Via le script Python : Lancez `python code/load_model.py` depuis la racine du projet.
---

## ğŸ§‘â€ğŸ’» Authors

- [Zeineb Ghrib]: A constituÃ© le jeu de donnÃ©es PokÃ©mon et a participÃ© Ã  la crÃ©ation du modÃ¨le CNN.

---

## ğŸ“š Sources
Les donnÃ©es utilisÃ©es pour ce projet ont Ã©tÃ© recueillies Ã  partir des plateformes suivantes, avec des spÃ©cificitÃ©s pour chaque dataset :

- Google Images: UtilisÃ© pour le scraping du dataset PokÃ©mon.
- Anime Characters Database: Source utilisÃ© pour les datasets Dragon Ball et Fairy Tail.
